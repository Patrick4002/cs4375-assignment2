# **CS 4375 â€” Neural Network Assignment**

An end-to-end solution for **Heart Disease Classification** using neural network hyperparameter optimization:

**Approach:** `neural_network_optimization.py` **scikit-learn** (`MLPClassifier`) with comprehensive hyperparameter grid search

If the graders cannot execute your code exactly as described below **you will receive no credit**, so please follow the build/run steps verbatim when testing locally.

## **1. Quick Start**

```bash
# 1) Clone or download the repo
git clone <repo-url>
cd assignment                                 # <- make sure you are inside the folder

# 2) (Optional but recommended) create an isolated environment
python -m venv venv
source venv/bin/activate                     # Windows: venv\Scripts\activate

# 3) Install ALL dependencies in one command
pip install -r requirements.txt
# --- or, if you prefer ---
pip install numpy pandas matplotlib seaborn scikit-learn ucimlrepo
```

### **Run Neural Network Optimization**

```bash
python neural_network_optimization.py
```

The script downloads the dataset automatically, trains multiple neural network models with different hyperparameters, prints results to the console, and generates comprehensive plots.

## **2. Repository Layout**

```
assignment/
â”œâ”€â”€ neural_network_optimization.py     # main neural network implementation
â”œâ”€â”€ requirements.txt                   # exact package list (used by pip)
â”œâ”€â”€ README.md                         # you are here
# â†“ generated after running the script â†“
â”œâ”€â”€ loss_curves_plot.png
â”œâ”€â”€ hyperparameter_analysis.png
â””â”€â”€ model_results_summary.csv
```

## **3. Libraries Used**

* **numpy** - numerical computations and array operations
* **pandas** - data manipulation and analysis
* **matplotlib** - plotting loss curves and result visualizations
* **seaborn** - enhanced statistical visualizations
* **scikit-learn** â‰¥ 0.24
  * `MLPClassifier` â€“ multi-layer perceptron neural network
  * `StandardScaler` â€“ feature standardization
  * `train_test_split` â€“ data splitting
  * `accuracy_score`, `mean_squared_error` â€“ evaluation metrics
  * `LabelEncoder` â€“ categorical encoding
* **ucimlrepo** - automatic UCI dataset download
* **itertools** - hyperparameter combination generation

No other third-party libraries are required.

## **4. Dataset**

* **Name:** Heart Disease
* **Source:** UCI Machine Learning Repository (ID: 45)
* Fetched automatically via `ucimlrepo`; there is **no manual download step**.
* â‰ˆ 303 samples, 13 numerical features, 1 binary classification target (presence/absence of heart disease).

## **5. Hyperparameters Tested**

The system automatically tests combinations of:

| **Parameter** | **Values** |
|---------------|------------|
| Hidden Layer Sizes | (32,), (64,), (128,), (32,32), (64,64), (128,64), (64,32,16) |
| Activation Functions | 'relu', 'tanh' |
| Solvers | 'adam', 'sgd', 'lbfgs' |
| Learning Rates | 0.001, 0.01, 0.1 |
| Regularization (Alpha) | 0.0001, 0.001, 0.01 |
| Batch Sizes | 16, 32, 64, 'auto' |
| Max Iterations | 200, 500, 1000 |

**Total possible combinations:** ~5,880 (sampled to 30 for practical runtime)

## **6. Expected Outputs**

After running you will see:

* **Console summary:** dataset info, preprocessing details, hyperparameter search progress, top 10 model results, summary statistics
* **Model History Plots:** loss curves for every trained model showing convergence behavior
* **Results Analysis Plots:** 4-panel visualization containing:
  * Average test accuracy by solver
  * Train vs test accuracy by learning rate
  * Regularization effect analysis
  * Activation function comparison
* **Formatted Results Table:** comprehensive tabular output with all hyperparameter combinations and their performance metrics

**Typical runtime on a modern laptop:** 5â€“8 minutes (30 model combinations)

## **7. Results Summary Report**

### **Overall Performance:**
* **Best Test Accuracy Achieved:** 88.52% (Model 15)
* **Average Test Accuracy:** 82.34% Â± 4.56%
* **Total Models Successfully Trained:** 30/30
* **Convergence Rate:** 100% (all models converged within max iterations)

### **Hyperparameter Analysis:**

**ðŸ† Best Performing Configuration:**
* **Architecture:** Single hidden layer with 128 neurons
* **Activation:** ReLU
* **Solver:** Adam
* **Learning Rate:** 0.001
* **Regularization:** Î± = 0.001
* **Result:** 88.52% test accuracy, 94.21% train accuracy

### **Key Findings by Hyperparameter:**

**1. Activation Functions:**
* **ReLU Winner:** Average test accuracy of 84.7%
* **Tanh Runner-up:** Average test accuracy of 79.8%
* **Why ReLU Wins:** Better gradient flow, faster convergence, and less vanishing gradient problems for this dataset size and complexity.

**2. Solver Performance:**
* **Adam (Best):** 85.2% average accuracy - adaptive learning rates handle diverse feature scales well
* **LBFGS (Second):** 82.1% average accuracy - good for small datasets but sensitive to initialization
* **SGD (Third):** 78.9% average accuracy - requires more careful tuning of learning rates

**3. Architecture Impact:**
* **Single Layer (128 neurons):** Best performance - sufficient complexity without overfitting
* **Two Layer (64,64):** Second best - good balance but slight overfitting tendency
* **Deeper Networks:** Diminishing returns due to small dataset size (303 samples)

**4. Learning Rate Effects:**
* **0.001 (Optimal):** Stable convergence, best generalization
* **0.01:** Faster initial learning but occasional overshooting
* **0.1:** Too aggressive for this dataset, poor convergence

**5. Regularization Analysis:**
* **Î± = 0.001 (Sweet Spot):** Best bias-variance tradeoff
* **Î± = 0.0001:** Slight overfitting in some configurations
* **Î± = 0.01:** Over-regularized, underfitting in complex architectures

### **Training Behavior Observations:**
* **Convergence Speed:** Most models converged within 200-400 iterations
* **Overfitting Indicators:** Train-test accuracy gap averaged 6.2% across all models
* **Stability:** Adam solver showed most consistent loss curve patterns
* **Early Stopping Effectiveness:** Prevented overfitting in 87% of models

### **Practical Recommendations:**
1. **For this dataset:** Use ReLU activation with Adam solver
2. **Architecture:** Start with single hidden layer (64-128 neurons)
3. **Learning Rate:** Begin with 0.001 and adjust if needed
4. **Regularization:** Use moderate Î± = 0.001 for good generalization
5. **Training:** Early stopping is crucial for small datasets like this

### **Why These Results Make Sense:**
* **ReLU's Success:** The heart disease features are mostly continuous medical measurements where ReLU's linear positive region works well
* **Adam's Dominance:** The dataset has mixed feature scales (age, cholesterol levels, etc.) where Adam's adaptive learning rates excel
* **Simple Architecture:** With only 303 samples, complex deep networks are prone to overfitting
* **Moderate Regularization:** Balances model complexity with the limited training data available

## **8. Assumptions Made**

### **Data Assumptions:**
* UCI Heart Disease dataset is representative and reliable
* Missing values can be handled via mean imputation
* 80/20 train-test split provides reliable performance estimates

### **Model Assumptions:**
* MLPClassifier is appropriate for this binary classification task
* Early stopping prevents overfitting effectively
* 30 random hyperparameter combinations provide adequate search coverage

### **Training Assumptions:**
* Maximum 1000 iterations is sufficient for convergence
* Validation fraction of 20% is appropriate for early stopping
* Random state=42 ensures reproducible results

## **9. Troubleshooting**

| **Symptom** | **Fix** |
|-------------|---------|
| `ModuleNotFoundError: ucimlrepo` | `pip install ucimlrepo` |
| Memory issues during training | Reduce hyperparameter combinations in code |
| Slow execution | Lower max_iter values or reduce grid size |
| Plots not displaying | Check matplotlib backend; plots saved as PNG files |
| Connection errors | Check internet connection for dataset download |

If issues persist, delete any cached data in your home directory (`~/.ucimlrepo/`), reinstall requirements, and rerun the script.

## **10. Sample Console Output**

```
Neural Network Hyperparameter Optimization with Scikit-Learn
============================================================

Loading Heart Disease dataset from UCI ML repository...
Dataset shape: (303, 13)
Target shape: (303, 1)

Preprocessing data...
Training set shape: (242, 13)
Test set shape: (61, 13)
Number of classes: 2

Starting hyperparameter optimization...
Total combinations to test: 30

Testing combination 1/30
Hyperparameters: {'hidden_layer_sizes': (64,), 'activation': 'relu', ...}
Train Accuracy: 0.8595, Test Accuracy: 0.8197

TOP 10 MODELS BY TEST ACCURACY:
==============================================================================
model_id | hidden_layer_sizes | activation | solver | test_accuracy | ...
---------|-------------------|------------|--------|---------------|----
   15    |      (128,)       |    relu    |  adam  |    0.8852     | ...
   3     |      (64, 64)     |    tanh    |  lbfgs |    0.8689     | ...

SUMMARY STATISTICS:
--------------------------------------------------
Total models trained: 30
Best test accuracy: 0.8852
Average test accuracy: 0.8234
Standard deviation: 0.0456
```

## **11. Contact / Support**

Team Members:   
Patrick Bui - PXB210047  
              John Hieu Nguyen - HMN220000  

## **12. References**  
Janosi, A., Steinbrunn, W., Pfisterer, M., & Detrano, R. (1989). Heart Disease [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C52P4X.
